# **Arquitectura de Procesamiento Batch en GCP**  

**Descripci贸n**: Este repositorio contiene los scripts y configuraci贸n para un pipeline de ETL en Google Cloud Platform (GCP) que procesa datos de ventas de autom贸viles.  

---

## ** Arquitectura**  
```mermaid
graph LR
    A[CSV Local] --> B[Cloud Storage]
    B --> C[Cloud Data Fusion]
    B --> D[Dataproc PySpark]
    C --> E[BigQuery]
    D --> E[BigQuery]
    E --> F[Looker Studio]
```

Este pipeline aprovecha m煤ltiples servicios de GCP para permitir un flujo de datos eficiente, desde archivos CSV locales hasta visualizaciones interactivas.  

---

## **О Servicios Utilizados y Descripci贸n**  

### **1. Cloud Storage (GCS)**  
- **Descripci贸n**: Almacenamiento escalable de objetos. Aqu铆 se sube el archivo `autodata.csv`.  
- **Caracter铆sticas**: Alta durabilidad, f谩cil integraci贸n con otros servicios GCP, control de versiones.  
- **Costo aproximado**: $0.026 por GB almacenado al mes (Standard).  

### **2. Cloud Data Fusion**  
- **Descripci贸n**: Herramienta visual de integraci贸n de datos para construir pipelines ETL sin necesidad de c贸digo extensivo.  
- **Uso en este proyecto**: Se utiliz贸 para probar transformaciones de datos sin escribir c贸digo, y para contrastar su facilidad de uso frente a Dataproc.  
- **Costo aproximado**: Desde $1.80 por hora de instancia b谩sica (Developer Edition).  
- **Ventaja**: Ideal para usuarios no t茅cnicos y para orquestaci贸n r谩pida de ETL visualmente.  

### **3. Dataproc (PySpark)**  
- **Descripci贸n**: Servicio de procesamiento distribuido basado en Apache Spark.  
- **Uso en este proyecto**: Transformaci贸n de datos en paralelo usando PySpark (limpieza, c谩lculo de m茅tricas, particionamiento).  
- **Comando de ejecuci贸n desde Cloud Shell**:  
```bash
gcloud dataproc jobs submit pyspark     --cluster=mi-cluster-spark     --region=us-central1     --jars=gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.36.1.jar     --properties=spark.submit.deployMode=cluster     --service-account=dataproc-service-account@bigdata-batch-demo.iam.gserviceaccount.com     gs://auto-sales-bigdata/scripts/data_processing-dataproc.py
```
- **Costo aproximado**: ~$0.01 a $0.10 por hora por nodo, dependiendo del tipo de m谩quina.  

### **4. BigQuery**  
- **Descripci贸n**: Almac茅n de datos altamente escalable y sin servidor. Se utiliza SQL est谩ndar para consultar los datos.  
- **Caracter铆sticas**: Particionamiento por fecha (`ORDERDATE`), clustering por categor铆a (`DEAL_CATEGORY`), consultas r谩pidas a gran escala.  
- **Costo aproximado**:  
  - Almacenamiento: ~$0.02 por GB por mes.  
  - Consultas: ~$5 por TB procesado (primeros 1 TB/mes gratis).  

### **5. Looker Studio**  
- **Descripci贸n**: Herramienta gratuita de visualizaci贸n de datos. Permite crear dashboards conectados directamente a BigQuery.  
- **Costo**: Gratuito.  

---

## **锔 Configuraci贸n**  

### **1. Requisitos**  
- Cuenta en GCP con facturaci贸n habilitada.  
- Proyecto GCP con estos servicios activados:  
  - Cloud Storage  
  - Cloud Data Fusion  
  - Dataproc  
  - BigQuery  
- Python 3.8+ y librer铆as:  
  ```bash
  pip install pandas google-cloud-storage pyspark
  ```

### **2. Archivos Clave**  
| Archivo | Descripci贸n |  
|---------|-------------|  
| [`clean_data_fusion.py`](./clean_data_fusion.py) | Limpieza inicial del dataset (local). Soluciona un problema en Data Fusion con delimitadores de coma. |  
| [`Load-GCS.py`](./Load-GCS.py) | Establece conexi贸n y carga de datos a Cloud Storage. |  
| [`data_processing-dataproc.py`](./data_processing-dataproc.py) | Script PySpark usado en Dataproc con conexi贸n a BigQuery. |  

---

## ** Ejecuci贸n**  

### **1. Cargar Datos a GCS**  
```bash
python Load-GCS.py
```
**Salida esperada**:  
```
Archivo data/subset_limpio.csv subido a gs://auto-sales-bigdata/autodata.csv
```

### **2. Procesar con Dataproc**  
```bash
gcloud dataproc jobs submit pyspark     --cluster=mi-cluster-spark     --region=us-central1     --jars=gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.36.1.jar     --properties=spark.submit.deployMode=cluster     --service-account=dataproc-service-account@bigdata-batch-demo.iam.gserviceaccount.com     gs://auto-sales-bigdata/scripts/data_processing-dataproc.py
```
**Transformaciones aplicadas**:  
- Conversi贸n de fechas (`ORDERDATE`).  
- C谩lculo de valor total (`TOTAL_ORDER_VALUE`).  
- Clasificaci贸n por tama帽o de venta (`DEAL_CATEGORY`).  

---

## ** Resultados en BigQuery**  
Consulta de prueba:  
```sql
SELECT * FROM `bigdata-batch-demo.autodataset.sales_processed` LIMIT 10;
```
- **Particionamiento**: Por `ORDERDATE`.  
- **Clustering**: Por `DEAL_CATEGORY`.  
- **Consultas**: Usando SQL est谩ndar (SELECT, JOIN, GROUP BY, etc.).  


** M谩s recursos**:  
- [Documentaci贸n de Cloud Data Fusion](https://cloud.google.com/data-fusion/docs)  
- [Documentaci贸n de Dataproc](https://cloud.google.com/dataproc)  
- [Gu铆a de BigQuery](https://cloud.google.com/bigquery/docs)  
